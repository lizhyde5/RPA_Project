{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "label_test_template.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "B4yLzjne6Tm6",
        "NaSSY_mynUVw"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "81fa62675a5c4de0ab30b72158e49426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_00bc26236960449cb3c8740d5f0bf353",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_140078feac2b4d9aa0bfc6ba8c0180e4",
              "IPY_MODEL_663680d1e0e840d29e17a84cda4afefd"
            ]
          }
        },
        "00bc26236960449cb3c8740d5f0bf353": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "140078feac2b4d9aa0bfc6ba8c0180e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c1612a1686fc4fa1bcedc33e36ea8175",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f5332abfae774c07bf8a74ca96996c82"
          }
        },
        "663680d1e0e840d29e17a84cda4afefd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_23fe99e847034cbabcc1c08785822043",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:07&lt;00:00, 59.2B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d07bac99335f4d3c855ab59f244d42e8"
          }
        },
        "c1612a1686fc4fa1bcedc33e36ea8175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f5332abfae774c07bf8a74ca96996c82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "23fe99e847034cbabcc1c08785822043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d07bac99335f4d3c855ab59f244d42e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bf8ee667b3534e34b901795ec54889c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5c76984b2d4a492cb86cf00d74be9182",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_de047bae1be54b1d8a98c3af33781975",
              "IPY_MODEL_61248ccc0aa04c8b847be7f282eec94f"
            ]
          }
        },
        "5c76984b2d4a492cb86cf00d74be9182": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de047bae1be54b1d8a98c3af33781975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bcfdf9526f614f24b74f9f6de5c5a72a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_52955eaef21e4a07bededbf7a32ca183"
          }
        },
        "61248ccc0aa04c8b847be7f282eec94f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d9d651b8516944d4ac4ac8e8ddab043e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:07&lt;00:00, 61.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eca20fa43b004a91812b887ea9bbd32c"
          }
        },
        "bcfdf9526f614f24b74f9f6de5c5a72a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "52955eaef21e4a07bededbf7a32ca183": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d9d651b8516944d4ac4ac8e8ddab043e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eca20fa43b004a91812b887ea9bbd32c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dece57120a824e948e7275b662cdacf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9f6dc998348d4c39810c3dd12e66373e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ae7c068ac58d4da0bbe36ab9b7f578ce",
              "IPY_MODEL_b1ed815bb1404bea9dfbc6adcdeee6e3"
            ]
          }
        },
        "9f6dc998348d4c39810c3dd12e66373e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ae7c068ac58d4da0bbe36ab9b7f578ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8393a2af07cb4f4683956603d6bdc0e5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_df97793c014d469ebfac8d2318e25c62"
          }
        },
        "b1ed815bb1404bea9dfbc6adcdeee6e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_28247ffd24dc40c497f36b457abc681b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 658kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2dc158c404cc48c0884f341ae2060b82"
          }
        },
        "8393a2af07cb4f4683956603d6bdc0e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "df97793c014d469ebfac8d2318e25c62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28247ffd24dc40c497f36b457abc681b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2dc158c404cc48c0884f341ae2060b82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-Bnqn00VGfY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "34b8ad2c-4ac7-4119-cfa7-4fd0bf7e91cc"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 778kB 8.1MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0MB 22.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 51.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 49.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=8162132839c4915c50c296e023284ffd433d1985ea615c204e5c30a40b795441\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67970XzEVCAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import transformers\n",
        "# per the setting of transformers, to use any of its NLP model\n",
        "# we need to have three things: that is BertTokenizer, BertModel, BertConfig\n",
        "from transformers import BertTokenizer, BertModel, BertConfig"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4socdVzVEHS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f54c114f-1563-464b-f9a4-34792e1f8747"
      },
      "source": [
        "# first, let's see if we have GPU so that we could train our model in GPU\n",
        "# GPU is really at parallel computation\n",
        "from torch import cuda\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print(device)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59RK1lkXU9NH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# collect Google Drive to Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2L80RjhU9s_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "81fa62675a5c4de0ab30b72158e49426",
            "00bc26236960449cb3c8740d5f0bf353",
            "140078feac2b4d9aa0bfc6ba8c0180e4",
            "663680d1e0e840d29e17a84cda4afefd",
            "c1612a1686fc4fa1bcedc33e36ea8175",
            "f5332abfae774c07bf8a74ca96996c82",
            "23fe99e847034cbabcc1c08785822043",
            "d07bac99335f4d3c855ab59f244d42e8",
            "bf8ee667b3534e34b901795ec54889c2",
            "5c76984b2d4a492cb86cf00d74be9182",
            "de047bae1be54b1d8a98c3af33781975",
            "61248ccc0aa04c8b847be7f282eec94f",
            "bcfdf9526f614f24b74f9f6de5c5a72a",
            "52955eaef21e4a07bededbf7a32ca183",
            "d9d651b8516944d4ac4ac8e8ddab043e",
            "eca20fa43b004a91812b887ea9bbd32c"
          ]
        },
        "outputId": "49f87cf3-2c38-4817-c991-1c79d3d350b0"
      },
      "source": [
        "# create an instance of BERT model \n",
        "# note that it takes time to download the BERT model (~ 440M)\n",
        "# BERT model is big, because it has a lot of paramters. \n",
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81fa62675a5c4de0ab30b72158e49426",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf8ee667b3534e34b901795ec54889c2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKo8Z3XUYKJ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "dece57120a824e948e7275b662cdacf3",
            "9f6dc998348d4c39810c3dd12e66373e",
            "ae7c068ac58d4da0bbe36ab9b7f578ce",
            "b1ed815bb1404bea9dfbc6adcdeee6e3",
            "8393a2af07cb4f4683956603d6bdc0e5",
            "df97793c014d469ebfac8d2318e25c62",
            "28247ffd24dc40c497f36b457abc681b",
            "2dc158c404cc48c0884f341ae2060b82"
          ]
        },
        "outputId": "15db57a6-33e1-44d0-9d2b-86b0f7ba2ea2"
      },
      "source": [
        "# creat an instance of BERT tokenizer\n",
        "# as you can tell, the tokenizer is pretty small, only 232k in size\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dece57120a824e948e7275b662cdacf3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyhRDyuDUGwu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "outputId": "80ea0e1c-56bb-4a14-ee8f-3a1ef451ec0b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/lizhyde5/RPA_Project/master/16K_AmazonReviews.csv'\n",
        "raw_review = pd.read_csv(url)\n",
        "raw_review"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>product_title</th>\n",
              "      <th>product_category</th>\n",
              "      <th>star_rating</th>\n",
              "      <th>helpful_votes</th>\n",
              "      <th>total_votes</th>\n",
              "      <th>vine</th>\n",
              "      <th>verified_purchase</th>\n",
              "      <th>review_headline</th>\n",
              "      <th>review_body</th>\n",
              "      <th>review_date</th>\n",
              "      <th>social_connectedness</th>\n",
              "      <th>environment</th>\n",
              "      <th>self_sufficiency</th>\n",
              "      <th>transparency_authenticity</th>\n",
              "      <th>tradition</th>\n",
              "      <th>individuality</th>\n",
              "      <th>diversity_equality</th>\n",
              "      <th>privacy</th>\n",
              "      <th>status</th>\n",
              "      <th>thrift_value</th>\n",
              "      <th>innovation</th>\n",
              "      <th>fun_adventure</th>\n",
              "      <th>health</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BOSS HUGO BOSS Men's Starfish Swim Trunk</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Good quality</td>\n",
              "      <td>The quality is good, fits good but I didnt pay...</td>\n",
              "      <td>6/23/2014</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Vedette Megane Firm Compression Sensual Corset...</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>I like this bodyshaper.</td>\n",
              "      <td>This body shaper does what it says. It makes y...</td>\n",
              "      <td>9/1/2013</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Adult Stilinski 24 Beacon Hills Lacrosse 2-Sid...</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Nice hoodie</td>\n",
              "      <td>It looks well made. I bought it for my grandda...</td>\n",
              "      <td>8/3/2015</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Carhartt Men's Sherpa Lined Sandstone Hooded M...</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Size Small Means Medium</td>\n",
              "      <td>Other than the sizing that is bigger than the ...</td>\n",
              "      <td>10/2/2014</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Roxy Juniors Gallery Backpack</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>Love my backpack.</td>\n",
              "      <td>9/15/2014</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1656</th>\n",
              "      <td>Virus Removal Service - Jupiter Support</td>\n",
              "      <td>Software</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>IE issue</td>\n",
              "      <td>i just got done with jupiter on i script prob ...</td>\n",
              "      <td>4/26/2014</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1657</th>\n",
              "      <td>Family Tree Maker 2011 Deluxe [Old Version]</td>\n",
              "      <td>Software</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>Not for use on a netbook</td>\n",
              "      <td>I used this software on my netbook and it divi...</td>\n",
              "      <td>9/23/2011</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1658</th>\n",
              "      <td>Learn Italian: Fluenz Italian 1 for Mac, PC, i...</td>\n",
              "      <td>Software</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>awesome program</td>\n",
              "      <td>hi, Just want to say it what a great company! ...</td>\n",
              "      <td>4/30/2010</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1659</th>\n",
              "      <td>Kaspersky Internet Security 2009 (3 User)</td>\n",
              "      <td>Software</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Great product</td>\n",
              "      <td>Great internet security product. I guess the b...</td>\n",
              "      <td>12/22/2008</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1660</th>\n",
              "      <td>Norton 360 2013 - 1 User / 3 PC</td>\n",
              "      <td>Software</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Disapointing right out of the box</td>\n",
              "      <td>If you are allergic to the chocolate and you a...</td>\n",
              "      <td>9/12/2013</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1661 rows Ã— 23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          product_title  ... health\n",
              "0              BOSS HUGO BOSS Men's Starfish Swim Trunk  ...      0\n",
              "1     Vedette Megane Firm Compression Sensual Corset...  ...      1\n",
              "2     Adult Stilinski 24 Beacon Hills Lacrosse 2-Sid...  ...      0\n",
              "3     Carhartt Men's Sherpa Lined Sandstone Hooded M...  ...      1\n",
              "4                         Roxy Juniors Gallery Backpack  ...      0\n",
              "...                                                 ...  ...    ...\n",
              "1656            Virus Removal Service - Jupiter Support  ...      0\n",
              "1657        Family Tree Maker 2011 Deluxe [Old Version]  ...      0\n",
              "1658  Learn Italian: Fluenz Italian 1 for Mac, PC, i...  ...      0\n",
              "1659          Kaspersky Internet Security 2009 (3 User)  ...      0\n",
              "1660                    Norton 360 2013 - 1 User / 3 PC  ...      0\n",
              "\n",
              "[1661 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVLbEIyuWoWW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "d5506a5b-6cc0-4521-bb64-9fb67884848c"
      },
      "source": [
        "raw_review.iloc[398,:]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "product_title                      Women's Combed Cotton Rib-Knit Thermal Vest\n",
              "product_category                                                       Apparel\n",
              "star_rating                                                                  1\n",
              "helpful_votes                                                                0\n",
              "total_votes                                                                  0\n",
              "vine                                                                         N\n",
              "verified_purchase                                                            Y\n",
              "review_headline                    Women's Combed Cotton Rib-Knit Thermal Vest\n",
              "review_body                  it is not like it looks in the image!<br />i d...\n",
              "review_date                                                         11/23/2012\n",
              "social_connectedness                                                         0\n",
              "environment                                                                  0\n",
              "self_sufficiency                                                             0\n",
              "transparency_authenticity                                                    0\n",
              "tradition                                                                    0\n",
              "individuality                                                                0\n",
              "diversity_equality                                                           0\n",
              "privacy                                                                      0\n",
              "status                                                                       0\n",
              "thrift_value                                                                 1\n",
              "innovation                                                                   0\n",
              "fun_adventure                                                                0\n",
              "health                                                                       0\n",
              "Name: 398, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bO_VL6_GV6gF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_review = pd.DataFrame(None)\n",
        "\n",
        "train_review[['text','label']] = raw_review[['review_body','thrift_value']].iloc[0:1661,:]\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4F0PknWZqxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_review = train_review.dropna()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajQpmwt2Xfds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "4e5f0057-0ffb-4b22-d339-caa94888bc7b"
      },
      "source": [
        "train_review.describe()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1660.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.792771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.405443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             label\n",
              "count  1660.000000\n",
              "mean      0.792771\n",
              "std       0.405443\n",
              "min       0.000000\n",
              "25%       1.000000\n",
              "50%       1.000000\n",
              "75%       1.000000\n",
              "max       1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "513Gaudcu56u",
        "colab_type": "text"
      },
      "source": [
        "# Define a Customized Dataset Class and Setup the Dataloader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "luKm7Mh6vZi4",
        "colab": {}
      },
      "source": [
        "# first let's define some key parameters we will use later\n",
        "# note this is a relatively large number, making the training process slow, \n",
        "# but it's necessary becuase a lot of reviews are long.\n",
        "max_length = 128\n",
        "# how many raw inputs we feed into the train and validation model at once\n",
        "train_batch = 32\n",
        "valid_batch = 32\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "APMXoM-b81B4",
        "colab": {}
      },
      "source": [
        "# then we need to import the libraries we need\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5zOBRz6rvwfh",
        "colab": {}
      },
      "source": [
        "class YelpDataset(Dataset):\n",
        "\n",
        "  # here we want to  create a customized dataset--YelpDataset--which could take a raw text as input\n",
        "  # and encode it in BERT's way with a tokenizer. \n",
        "  # The tokenized input will be then fed into a BERT Model in the NN, which we will create later\n",
        "\n",
        "  # __init__ defines some necessary attributes for any instance created\n",
        "  # like: the dataset with raw text reviews, what tokenizer we want to use for encoding, \n",
        "  # the max length of sentence we need to pad or trancate\n",
        "  def __init__(self, dataset, tokenizer, max_len):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = dataset\n",
        "    self.text = dataset.text\n",
        "    self.label = dataset.label\n",
        "    self.max_len = max_len\n",
        "  \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # __getitem__ take index as input, \n",
        "    # in general/without customizatin, it returns the sample with the index in the dataset\n",
        "    # further we could customize its functionality, letting it to apply some operations on the \n",
        "    # index-specified sample before return a value for us\n",
        "    # here we use index to locate a specific review text we want to pre-process\n",
        "    # text = str(self.text[index])\n",
        "    # text = \" \".join(text.split())\n",
        "    text = self.text.iloc[index]\n",
        "\n",
        "    # then, we put the text into a BERT encode_plus\n",
        "    # important debug tips: \n",
        "    # for encode_plus, to keep all the raw inputs in the same lenght\n",
        "    # we need to specify BOTH padding and trancation in addition to max_length\n",
        "    # the former for short sentences and the latter for long ones\n",
        "    # failing to do either one will result in uneven lengths of encoded inputs,\n",
        "    # which will create troubles for your dataloader in the nn training sessions\n",
        "    outputs = self.tokenizer.encode_plus(\n",
        "        text,\n",
        "        max_length = self.max_len,\n",
        "        padding = \"max_length\",\n",
        "        truncation = \"longest_first\",\n",
        "        # note that in some versions of transformer in you local machine, the code is \n",
        "        # pad_to_max_length = True,\n",
        "        # truncation_strategy = 'longest_first',\n",
        "        # we might need to change the argument name a little to fit different version of transformers\n",
        "        add_special_tokens = True,\n",
        "        return_tensors = 'pt'\n",
        "    )\n",
        "    # recall the BERT Tokenizer session, it takes raw text as input\n",
        "    # and return input_ids, attention_mask\n",
        "    input_ids = outputs['input_ids']\n",
        "    attention_mask = outputs['attention_mask']\n",
        "    # we then store those values and put them together with the label info in the sample\n",
        "    # as the return of the __getitem__ method\n",
        "    return {\n",
        "        'input_ids': torch.tensor(input_ids, dtype = torch.long),\n",
        "        'attention_mask': torch.tensor(attention_mask, dtype = torch.long),\n",
        "        'label': torch.tensor(self.label.iloc[index], dtype = torch.float) \n",
        "    }\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tZLP1R7vBuKL",
        "colab": {}
      },
      "source": [
        "# split the train and validate dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_raw, valid_raw = train_test_split(train_review, test_size = 0.1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eRqVwGM5v2_d",
        "colab": {}
      },
      "source": [
        "# create instances of the YelpDataset for raw trianing and validate datasets\n",
        "# recall that tokenizer has be defined by: tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') in the BERT tokenizer session\n",
        "train_processed = YelpDataset(train_raw, tokenizer, max_length)\n",
        "valid_processed = YelpDataset(valid_raw, tokenizer, max_length)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eEl2Jkdksm4i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e93af76c-d68f-4624-f073-583b7ac04e0c"
      },
      "source": [
        "# check the attributes and method of train_processed\n",
        "train_processed.__len__()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1494"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s7HaY0bQCTXw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "3590523b-f15b-4e37-8a47-ed103b4a6d77"
      },
      "source": [
        "# test the customized Dataset instance\n",
        "print(train_processed.__getitem__(10))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[  101,  2023,  4007,  2573,  2004,  3517,  1012,  2026,  3274,  2038,\n",
            "          1996,  4185,  2978, 13151,  1025,  2061,  1045,  2134,  1005,  1056,\n",
            "          2031,  1996,  3291,  2060,  3080,  6097,  3274,  5608, 10865,  2055,\n",
            "          1012,  1012,  1012,  1045,  5993,  1010,  2027,  2323,  2031,  2070,\n",
            "          2828,  1997,  2200,  5793,  5432,  2055, 11303,  1996,  6745, 18017,\n",
            "          2000,  2224,  2023,  4007,  1012,  2026,  2069, 12087,  2003,  1012,\n",
            "          1012,  1012,  2205,  2116,  3980,  2055,  2477,  2008,  2134,  1005,\n",
            "          1056,  6611,  2000,  2033,  1998,  2644,  2667,  2000,  5271,  2033,\n",
            "          4933,  1999,  1996,  2690,  1997,  2667,  2000,  2131,  2083,  1996,\n",
            "          5409,  2112,  1997,  2026,  2095,   999,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]]), 'label': tensor(1.)}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hvnlzcFRRcxw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "0cb8d46c-0b93-4721-cf0a-0f9ab5ec2cc1"
      },
      "source": [
        "# see the dimension of the pre-processed data\n",
        "print(train_processed.__getitem__(10)['input_ids'].shape)\n",
        "# we can use the squeeze() method to remove the axis of \"1\", a method we will use later\n",
        "print(train_processed.__getitem__(10)['input_ids'].squeeze().shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 128])\n",
            "torch.Size([128])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k4T7kvxJEpZX",
        "colab": {}
      },
      "source": [
        "# important, run it again to create a new dataset loader for every NN model you train\n",
        "# lastly, set up the dataloader that serves as a pipeline feeding pre-processed data into the neural network\n",
        "train_sampler = RandomSampler(train_processed)\n",
        "train_loader = DataLoader(train_processed, batch_size = train_batch, num_workers = 0)\n",
        "valid_sampler = SequentialSampler(valid_processed)\n",
        "valid_loader = DataLoader(valid_processed, batch_size = valid_batch, num_workers = 0)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gl3urqqxHeH9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c4226b8b-13c2-47a1-882f-6c3a08801b8f"
      },
      "source": [
        "# exploer the attributes of dataloder\n",
        "# length of dataloader = len(dataset)/batch size\n",
        "print(len(train_loader))\n",
        "print(len(valid_loader))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4yLzjne6Tm6",
        "colab_type": "text"
      },
      "source": [
        "# Define a Customized Neural Network with the First Hidden Layer as NLP Encoding Layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hLuI0Q3ZF2C2"
      },
      "source": [
        "Up to now, we have defined the customized Dataset class that preprocess the raw input, encoding them into input_ids and attention_mask that BERT model needs. We also set up the Dataloader that feed the pre-processed data into the Neural Network we are creating now. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oGetmmKNafv5",
        "colab": {}
      },
      "source": [
        "# import the functions we need use in the Netwrok Model\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ld8Oqz5Jv43c",
        "colab": {}
      },
      "source": [
        "# the neural network we create is a class called  YelpBERT, which inherits\n",
        "# the attributes and structures of the torch.nn.Module\n",
        "\n",
        "# the __init__ functino defines the necessary hidden layers of the NN\n",
        "# and the forward function set up the computation graph: the real calculation procedures of the NN\n",
        "# see the class session 7's slides and recording for details \n",
        "\n",
        "class YelpBERT(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(YelpBERT, self).__init__()\n",
        "    # recall that, we have defined model as: model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    # see BERT Model's input and output: https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "    self.l1 = model\n",
        "\n",
        "    # with this layer a percentage of all neurons will be randomly turned-off to prevent overfitting\n",
        "    # since our training dataset might be small, we really cannot afford a large neural network\n",
        "    # the output of the BERT layer is a vector of 768 elements, if fully connected to next linear layer\n",
        "    # then the current layer would have 768 neurons, too much! \n",
        "    # using dropout mechanism, we can randomly turn off a percentage of the neuraon in the training process\n",
        "    # literally reduced the number of neuron in the layer\n",
        "    self.l2 = torch.nn.Dropout(0.3) \n",
        "\n",
        "    # if you want to have multiple fully connected linear layer, use this\n",
        "    # self.l2 = torch.nn.Linear(768, 10)\n",
        "    # but again, if your training dataset is not very large, we may only offard one linear layer\n",
        "    \n",
        "    # last layer\n",
        "    self.l3 = torch.nn.Linear(768, 1)\n",
        "\n",
        "\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    # first layer\n",
        "    # the first layer utilize the BERT model (call \"model\" in __init__) to transfer the input_ids into\n",
        "    # contextualized word embeddings --numerical vectors\n",
        "    # we can customize the output of this layer such as using the [CLS] token or the mean of all input tokens\n",
        "   \n",
        "    # if you want to use the BERT output of the last self-attention layer, use this:\n",
        "    last, pooler = self.l1(input_ids = input_ids, attention_mask = attention_mask)\n",
        "   \n",
        "    # if you want to use BERT output from OTHER self-attention layers,use this:\n",
        "    # (note that BERT base model has 12 hidden self-attention layers), \n",
        "    # last, pooler, all = self.l1(input_ids = input_ids, attention_mask = attention_mask, output_hidden_states = True)\n",
        "    \n",
        "    # second layer\n",
        "    # here we could use the mean value of tokens of all raw inputs as the embedding of the whole input text\n",
        "    # and feed it into the second layer, use this: \n",
        "    # output from last BERT self-attention layer:\n",
        "    # initial index \"0\" for mean value of [cls] token and all non-padded tokens\n",
        "    # initial index \"1\" for mean value of all non-padded tokens only\n",
        "    output = last[:, 0 : attention_mask.sum(), :].mean(dim = 1)\n",
        "    # BERT output from other hidden self-attention layer:\n",
        "    # output = all[11][:, 0 : attention_mask.sum(), :].mean(dim = 1)\n",
        "    \n",
        "    # or use the [CLS] token of last layer\n",
        "    # output = last[:, 0, :]\n",
        "\n",
        "    # output from BERT model now be fed into a relu activation funcation adn\n",
        "    # the second layer of the Neural Network\n",
        "    output = F.relu(output.squeeze())\n",
        "    output = F.relu(self.l2(output))\n",
        "\n",
        "    # why squeeze(): to make the dimension of input-output across layers consistent\n",
        "    # e.g., the output of layer 1--self.l1, is in the shape of (1, 768),\n",
        "    # we use .squeeze() to make it in a shape of (768) only, \n",
        "    # because the later layer--l2 adn l2--take (768) as input dimension not (1, 768)\n",
        "\n",
        "    # third layer\n",
        "    output = self.l3(output)\n",
        "\n",
        "    # last sigmoid layer to furhter transfer the single scalar of l3 into a probability\n",
        "    return torch.sigmoid(output)\n",
        "    # note that we can also customize the layers after the 1st one, \n",
        "    # making more layers (i.e., a deeper NN) and see if it perform betters "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m8yf6W_iSdCJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "64cb3d25-a9de-438a-e228-330e4a65f420"
      },
      "source": [
        "# test the model step-by-step\n",
        "# compare the output dimension to your expectation\n",
        "# https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "# testing each layer in the NN is very important\n",
        "# we should pay close attention the the dimensions of the inputs and outputs of each layer \n",
        "# use .squeeze() to remove uncessary axis whose length is 1 to make the dimensions consistent. like \"output = pooler.squeeze()\" in the NN above\n",
        "l1 = model\n",
        "l1.to(device)\n",
        "input_ids = train_processed.__getitem__(10)['input_ids'].to(device)\n",
        "attention_mask = train_processed.__getitem__(10)['attention_mask'].to(device)\n",
        "last, pooler = l1(input_ids = input_ids, attention_mask = attention_mask)\n",
        "print(last.shape)\n",
        "print(last.squeeze().shape)\n",
        "print(pooler.shape)\n",
        "print(pooler.squeeze().shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 128, 768])\n",
            "torch.Size([128, 768])\n",
            "torch.Size([1, 768])\n",
            "torch.Size([768])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zuR1CGxHpNDt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "46ab3e44-68ee-4554-a364-bed2eef24c95"
      },
      "source": [
        "# similarly, test if the dimensions of input and output \n",
        "output = last[:, 1 : attention_mask.sum(), :].mean(dim = 1)\n",
        "output = output.squeeze()\n",
        "output.shape\n",
        "l2 = torch.nn.Dropout(0.5)   \n",
        "l2.to(device)\n",
        "l3 = torch.nn.Linear(768, 1)\n",
        "l3.to(device)\n",
        "output.to(device)\n",
        "l3(l2(torch.tensor(output)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4963], device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NaSSY_mynUVw"
      },
      "source": [
        "# Create a BERT + NN Model as an Instance of the Customized Model Classs Defined Above and Setup the Loss Function and Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aFDDnyD6wlD4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f2524078-64a2-48da-91f9-220027389fd2"
      },
      "source": [
        "# create an instance of the YelpBERT model\n",
        "# remember to recreat a instance of the Model Class after you modified the YelpBERT class\n",
        "model_yelp = YelpBERT()\n",
        "model_yelp.to(device)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "YelpBERT(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.3, inplace=False)\n",
              "  (l3): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2BnImCNBLxG_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0268990b-91c0-4e9f-fb22-9587342359d4"
      },
      "source": [
        "# test the model_yelp\n",
        "# recall that the train_processed is a special class that could pre-process the raw input words into input_ids\n",
        "# and attentino_mask using its method __getitem__ method\n",
        "# we then use this method to get the input_ids and attention_mask that we need to feed in the YelpBERT() model\n",
        "\n",
        "test_output = model_yelp(input_ids, attention_mask)\n",
        "print(len(test_output))\n",
        "print(test_output)\n",
        "# note that the output is a scalar, becasue ofthe last hidden layer in the NN, self.l3 = torch.nn.Linear(768, 1)\n",
        "# the scalar then can be put into a softmax for prediction \n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "tensor([0.5529], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7QmZRjGkPVAI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b87afbf7-904a-4c16-9b6f-dd8eadbfcee7"
      },
      "source": [
        "print(input_ids.shape)\n",
        "print(attention_mask.shape)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 128])\n",
            "torch.Size([1, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nrrzk6ukepKO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "efd8349c-cacd-4eb3-ad22-7626bbdc49f0"
      },
      "source": [
        "# check out the model parameters\n",
        "# note that the first BERT layer, the word embeddings are in the shape of (30522, 768)\n",
        "# and we can tell that the first 12 layers of our model are normal BERT layers, \n",
        "# the last two are what we customized-- a dropout layer and the linear layer\n",
        "model_yelp.parameters"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of YelpBERT(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.3, inplace=False)\n",
              "  (l3): Linear(in_features=768, out_features=1, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4enOR93aXnCG"
      },
      "source": [
        "Up to now, we have the Dataset (pre-process/encode the raw text inputs, making them into input_ids that BERT model takes as input), the Dataloader (feed the processed data into the NN in batchs), the neural network (with the first layer as a BERT model, the transfer the encoded input_ids into word embeddings, and later layers just work on those embeddings/numerical vectors as normal neural network). \n",
        "\n",
        "And finally, we could set up our training loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iZpo8C-zc-B3"
      },
      "source": [
        "First we start with: the loss function (e.g., crossentropy loss or mean squared error loss) and the optimize schedule (some thing about learning rate, adaptive learning rate, see class 7)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7TRC6Dbzc8oZ",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q4QtC037dzxO",
        "colab": {}
      },
      "source": [
        "# criterion is the loss function we use\n",
        "# here we use Binary CrossEntropy Loss, you can try others see the performance difference\n",
        "# https://pytorch.org/docs/master/generated/torch.nn.BCELoss.html\n",
        "criterion = nn.BCELoss()\n",
        "# note that loss function in pytorch framework usually take the pair of (prediction, ground_truth) as input\n",
        "# and give the loss value as output"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d0MvxRbleYiO",
        "colab": {}
      },
      "source": [
        "# optimizer is our optimize strategy, here we use stachastic gradient descending as the approch \n",
        "# to update our model parameters. \n",
        "# tips: previously I trained multiple models but with learning rate = 1e-05, 3e-05, \n",
        "# after 4 epochs the model performance almost doesn't change\n",
        "# thus now I use 10e-05, it seems the performance improves faster\n",
        "learning_rate = 10e-05\n",
        "# SGD is a common optimizer, but let's use Adam here, AdamW is a optimizer developed by Huggingface using Adam's mechanism\n",
        "# optimizer = optim.SGD(model_yelp.parameters(), lr = learning_rate, momentum = 0.9)\n",
        "# http://deeplearning.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/\n",
        "# you can try out other optimization method and see performance difference"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "njBahjNyjByc",
        "colab": {}
      },
      "source": [
        "# set the epoch\n",
        "epochs = 5\n",
        "# epochs means how many rounds each training sample will be fed into the NN.\n",
        "# next, we need to supply our model \"model_yelp\" to the GPU, so it can be run on GPU\n",
        "# model_yelp.to(device)\n",
        "# since the GPU has a lot of cores, it takes some time to supply to model to the GPU\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEUQBFhMhsHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try another optimizer\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "optimizer = AdamW(model_yelp.parameters(),lr = learning_rate,eps = 1e-8)\n",
        "                  \n",
        "total_steps = len(train_loader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler that update the learning rate gradully, this scheduler is with AdamW\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WUy2sgBBgz50",
        "colab": {}
      },
      "source": [
        "# lastly, let's define a helper function we can use for calculating the prediction accuracy\n",
        "import numpy as np\n",
        "# the idea of the funtion is that a vector of predicted probablity of being good review, \n",
        "# which is the output of the sigmoid/last layer of the neural network, is compared with the true label\n",
        "# is the predicted probability >= 0.5, we assign 1, otherwise we assign 0, we use np.around() achieve this\n",
        "def pred_accuracy (prediction, label):\n",
        "  pred_flat = np.around(prediction).flatten()\n",
        "  label_flat = label.flatten()\n",
        "  return np.sum(pred_flat == label_flat) / len(label_flat)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2WFSgu6_1VL",
        "colab_type": "text"
      },
      "source": [
        "# Train the BERT + NN Model and Evaluate the Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DFPc5CRqiESW",
        "colab": {}
      },
      "source": [
        "# set the random set the same, making the results reproducible\n",
        "import random\n",
        "seed_val = 45\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_SCqZW_Dn6x5"
      },
      "source": [
        "OK, finally, we strat to train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpYKGk-NeCUZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "87e31c73-f124-4a81-a13c-7265f59ab0b2"
      },
      "source": [
        "# create an instance of the YelpBERT model\n",
        "model_yelp = YelpBERT()\n",
        "model_yelp.to(device)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "YelpBERT(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.3, inplace=False)\n",
              "  (l3): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a417dCiz3VWS",
        "colab": {}
      },
      "source": [
        "# set up the directory for storing the trained model\n",
        "import os\n",
        "# save to Google Drive\n",
        "dir = \"/content/drive/My Drive\"\n",
        "# save to local file\n",
        "# dir = \"E://OneDrive - lmu.edu//Python Projects//BERT and ML\"\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H0gMiqrmix5g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "1de313a4-2da5-4fb9-8620-b64aebd92164"
      },
      "source": [
        "# Very Important: Each Time When You Train a Neural Network Again, \n",
        "# Plase recreate the nural network, the dataloader, and the optimizer and its scheduler.\n",
        "\n",
        "\n",
        "# train the model \n",
        "# store the total loss and accuracy values of each epoch\n",
        "training_stats = []\n",
        "\n",
        "# the epoch loop, number of rounds specified by epochs\n",
        "for epoch_i in range(0, epochs):\n",
        "  print('======== Epoch {:} / {:} =========='.format(epoch_i + 1, epochs))\n",
        "  # zero the values of total loss and accuracy of the epoch\n",
        "  total_loss = 0\n",
        "  total_accuracy = 0\n",
        "  # in the training stage, set the model into train mode\n",
        "  model_yelp.train()\n",
        "\n",
        "  # the training step loop\n",
        "  # recall that train_loader feed data into the NN in batchs to save memory and improve efficiency\n",
        "  # then the # of total steps is about (# of samples/batch size)\n",
        "  print(\"training\")\n",
        "  for step, batch in enumerate(train_loader, 0):\n",
        "    # feed the data into GPU using .to(device) method\n",
        "    # note that the input_ids for one raw input is in the shape of (1, max_length)\n",
        "    # after be processed into batch, the input_ids become (batch_size, 1, max_length)\n",
        "    # however, BERT only takes input_ids in the shape of (batch_size, max_length),\n",
        "    # (see here: https://huggingface.co/transformers/model_doc/bert.html#bertmodel)\n",
        "    # so we need to do \"batch['input_ids'].squeeze()\" inestead of \"batch['input_ids']\"\n",
        "    # to remove the unnecessy axis of \"1\". \n",
        "    # same operation for attention_mask\n",
        "    input_ids = batch['input_ids'].squeeze().to(device, dtype = torch.long)\n",
        "    attention_mask = batch['attention_mask'].squeeze().to(device, dtype = torch.long)\n",
        "    label = batch['label'].to(device, dtype = torch.float)\n",
        "\n",
        "    # at each step, before the NN does the feed forward, let's set the gradient to 0\n",
        "    # as pytorch nn.Module automatically cumulates gradient from previous rounds\n",
        "    # this is good for RNN training, but not necessy for us here. Thus, we turn it off.   \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # for each raw input, the feed forward calculation give us two scalar, \n",
        "    # representing the score/probability of the sample being 0/bad review class or 1/good review class  \n",
        "    # note that since we feed the inputs/raw samples in batch, \n",
        "    # the prediciton should be in the shape of (batch_size, # of classes)\n",
        "    prediction = model_yelp(input_ids, attention_mask)\n",
        "    prediction = prediction.squeeze()\n",
        "\n",
        "    # criterion is defined as a cross entropy loss function\n",
        "    # it takes (prediction, ground truth) as input arugments\n",
        "    # the former should be in the shape of (batch_size, # of classes), \n",
        "    # the latter should be in the sahpe of (batch_szie, 1), \"1\" dimension records the true class id of the input\n",
        "    # https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html   \n",
        "    loss = criterion(prediction, label)\n",
        "\n",
        "    # with the loss, we can do back propagation to calculate the gradient\n",
        "    # very easy, just one line of code  \n",
        "    loss.backward()\n",
        "\n",
        "    # with the gradient, we can update the model paramters. \n",
        "    # recall how we define the optimizer in the above cell  \n",
        "    optimizer.step()\n",
        "    \n",
        "    # update the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # to keep tracking on the model performance, we accumulate the total loss in every epoch  \n",
        "    total_loss += loss.item()\n",
        "      \n",
        "    # for every 10 steps, we print out the epoce # and loss\n",
        "    # again, the max_length of raw text input is 256, relatively long than usual, thus it will take longer to train. \n",
        "    if step%10 == 0 and step != 0:\n",
        "      print(f'Epoch:{epoch_i + 1}, Total_Loss:{total_loss}, Average_Loss:{total_loss/step}')\n",
        "  \n",
        "  # calcualte and store the average training loss of each batch in the current epoch\n",
        "  avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "  \n",
        "\n",
        "  # now let's set up the validation loop, meaning the trained model above will be used to evaluate the sample in validation dataset\n",
        "  # note that this validation loop is in the same \"indent\" level as the training loop, and they both under the epoches loop\n",
        "  print(\"validating\")\n",
        "  # set the model now in the evaluation mode\n",
        "  model_yelp.eval()\n",
        "  # zero the values of total loss and accuracy\n",
        "  total_loss = 0\n",
        "  #total_accuracy = 0\n",
        "\n",
        "  # validation loop\n",
        "  for step, batch in enumerate(valid_loader, 0):\n",
        "    input_ids = batch['input_ids'].squeeze().to(device, dtype = torch.long)\n",
        "    attention_mask = batch['attention_mask'].squeeze().to(device, dtype = torch.long)\n",
        "    label = batch['label'].to(device, dtype = torch.float)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction = model_yelp(input_ids, attention_mask)\n",
        "        prediction = prediction.squeeze()\n",
        "        loss = criterion(prediction, label)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    if step%10 == 0 and step != 0:\n",
        "      print(f'Epoch:{epoch_i + 1}, Total_Loss:{total_loss}, Average_Loss:{total_loss/step}')\n",
        "\n",
        "  # calcualte and store the average training loss of each batch in the current epoch\n",
        "  avg_valid_loss = total_loss / len(valid_loader)\n",
        "\n",
        "  training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_valid_loss\n",
        "        }\n",
        "    )\n",
        "  \n",
        "\n",
        "\n",
        "  # lastly, at the end of each epoch, let's save the model for later use\n",
        "  # note that dir is defined before as \"./content/drive/My Drive\"  \n",
        "  #torch.save(model_yelp.state_dict(), os.path.join(dir, 'exper-epoch-{}.pt'.format(epoch_i)))\n",
        "\n",
        "print(\"training complete!\")\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======== Epoch 1 / 5 ==========\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Total_Loss:5.717357844114304, Average_Loss:0.5717357844114304\n",
            "Epoch:1, Total_Loss:10.19218823313713, Average_Loss:0.5096094116568566\n",
            "Epoch:1, Total_Loss:14.514244019985199, Average_Loss:0.4838081339995066\n",
            "Epoch:1, Total_Loss:18.685896456241608, Average_Loss:0.4671474114060402\n",
            "validating\n",
            "======== Epoch 2 / 5 ==========\n",
            "training\n",
            "Epoch:2, Total_Loss:4.527608245611191, Average_Loss:0.4527608245611191\n",
            "Epoch:2, Total_Loss:7.812741816043854, Average_Loss:0.39063709080219267\n",
            "Epoch:2, Total_Loss:10.6848354190588, Average_Loss:0.35616118063529334\n",
            "Epoch:2, Total_Loss:13.257733196020126, Average_Loss:0.3314433299005032\n",
            "validating\n",
            "======== Epoch 3 / 5 ==========\n",
            "training\n",
            "Epoch:3, Total_Loss:3.042593702673912, Average_Loss:0.3042593702673912\n",
            "Epoch:3, Total_Loss:4.724332839250565, Average_Loss:0.23621664196252823\n",
            "Epoch:3, Total_Loss:6.150029171258211, Average_Loss:0.2050009723752737\n",
            "Epoch:3, Total_Loss:7.762863051146269, Average_Loss:0.19407157627865673\n",
            "validating\n",
            "======== Epoch 4 / 5 ==========\n",
            "training\n",
            "Epoch:4, Total_Loss:1.406820621341467, Average_Loss:0.1406820621341467\n",
            "Epoch:4, Total_Loss:2.4682208001613617, Average_Loss:0.12341104000806809\n",
            "Epoch:4, Total_Loss:4.417611934244633, Average_Loss:0.14725373114148776\n",
            "Epoch:4, Total_Loss:6.033808868378401, Average_Loss:0.15084522170946002\n",
            "validating\n",
            "======== Epoch 5 / 5 ==========\n",
            "training\n",
            "Epoch:5, Total_Loss:1.2530963942408562, Average_Loss:0.12530963942408563\n",
            "Epoch:5, Total_Loss:1.67388323135674, Average_Loss:0.083694161567837\n",
            "Epoch:5, Total_Loss:2.4445743057876825, Average_Loss:0.08148581019292275\n",
            "Epoch:5, Total_Loss:2.8728705774992704, Average_Loss:0.07182176443748176\n",
            "validating\n",
            "training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B5soA_VPd2Ry",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "outputId": "2d976079-28bb-4c22-a167-6be1406638e6"
      },
      "source": [
        "# plot the train statistics stored in training_stats\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set(style = \"darkgrid\")\n",
        "\n",
        "sns.set(font_scale = 1.5)\n",
        "plt.rcParams['figure.figsize'] = [12, 6]\n",
        "\n",
        "df_stats = pd.DataFrame(data = training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1031673860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9zXX5Pdvw9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make sure to save the model performance statistics and the associated model configuration. \n",
        "# that is save the df_stats to csv file with a file name, say experiment 1\n",
        "df_stats.to_csv(os.path.join(dir, 'label_experiment_3.csv'))\n",
        "# save the key hyper-parameters of this training experiment, say experiment 1\n",
        "label_experiment_3_config = {\n",
        "    \"epochs\": epochs,\n",
        "    \"train_batch_size\" : train_batch,\n",
        "    \"valid_batch_size\" : valid_batch,\n",
        "    \"initial_learning_rate\": learning_rate,\n",
        "     \"max_sentence_length\": max_length,\n",
        "     \"loss_fucntion\": criterion,\n",
        "     \"optimizer\": optimizer,\n",
        "     # you need to manually type-in the following info\n",
        "     \"BERT output\": \"mean value of [cls] embeddings of non-padded token from the second to the last layer\",\n",
        "     \"activation function\": \"relu\",\n",
        "     \"dropout rate of BERT output\": model_yelp.l2,\n",
        "     \"# of fully connected linear layer\": 1,\n",
        "     \"dataset\": \"Yelp Review Balanced\",\n",
        "     \"comment\": \"sees not as good as the outputs of last layer\"\n",
        "}\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odvnYYhghsH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the experiment configurate assocaited with this experiment\n",
        "# note that if you click the file icon (the third vertical one on the far left)\n",
        "# you will see the save files, double click on them, you can see them.\n",
        "import csv\n",
        "with open(os.path.join(dir, 'label_experiment_3_config.csv'), 'w') as csv_file:  \n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in label_experiment_3_config.items():\n",
        "       writer.writerow([key, value])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WinIsU6zMpWt",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate the Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p9MQQ6yU9kLo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c9ac63f2-5204-4611-94b6-8af55f7890b5"
      },
      "source": [
        "# apply the trained model to the validation dataset\n",
        "# get the model predictions and compare the comparisons to the true labels\n",
        "model_yelp.eval()\n",
        "predictions, labels = [], []\n",
        "for step, batch in enumerate(valid_loader):\n",
        "  input_ids = batch['input_ids'].squeeze().to(device, dtype = torch.long)\n",
        "  attention_mask = batch['attention_mask'].squeeze().to(device, dtype = torch.long)\n",
        "  label = batch['label'].to('cpu').numpy()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    prediction = model_yelp(input_ids, attention_mask)\n",
        "\n",
        "  prediction = prediction.detach().cpu().numpy()\n",
        "  predictions.append(prediction)\n",
        "  labels.append(label)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tA38tbL6Qkv7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "291b9b3c-8789-4db7-d061-b401e42054b8"
      },
      "source": [
        "# call the helper function-- pred_accuacy to compute the prediction accuracy in each batch\n",
        "ac = []\n",
        "for i in range(len(predictions)):\n",
        "  ac_i = pred_accuracy(predictions[i], labels[i])\n",
        "  ac.append(ac_i)\n",
        "ac"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.875, 0.71875, 0.75, 0.71875, 0.75, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QaL7PHMQOvjZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "498ae79f-e2c4-495c-bca3-dbff8840dec1"
      },
      "source": [
        "# transfer the outcomes into np\n",
        "predictions = np.asarray(predictions)\n",
        "labels = np.asarray(labels)\n",
        "predictions[0]\n",
        "# note that now the outcomes are still stored in batches"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.99265796],\n",
              "       [0.9876747 ],\n",
              "       [0.99289566],\n",
              "       [0.99269426],\n",
              "       [0.9926508 ],\n",
              "       [0.9905719 ],\n",
              "       [0.99278545],\n",
              "       [0.9929536 ],\n",
              "       [0.9928954 ],\n",
              "       [0.9909887 ],\n",
              "       [0.4444267 ],\n",
              "       [0.19371367],\n",
              "       [0.9923666 ],\n",
              "       [0.98878354],\n",
              "       [0.99281436],\n",
              "       [0.99255824],\n",
              "       [0.9912839 ],\n",
              "       [0.9920758 ],\n",
              "       [0.98625517],\n",
              "       [0.9925021 ],\n",
              "       [0.9892869 ],\n",
              "       [0.9923145 ],\n",
              "       [0.93370634],\n",
              "       [0.9921808 ],\n",
              "       [0.992331  ],\n",
              "       [0.9925418 ],\n",
              "       [0.992841  ],\n",
              "       [0.99295205],\n",
              "       [0.9908262 ],\n",
              "       [0.1422755 ],\n",
              "       [0.992448  ],\n",
              "       [0.87144285]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWiqZYkAhsID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c06809c-8ed7-4710-80bb-7bbd85be3366"
      },
      "source": [
        "# convert predictions stored in the batches into a long vector\n",
        "pred = np.concatenate(predictions, axis=0 )\n",
        "pred = np.concatenate(pred, axis=0 )\n",
        "pred = pred.reshape(len(pred),1)\n",
        "print(pred.shape)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(166, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5kkRWD0hsIE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "564c4336-0e44-4ea1-addd-d2d4c268233c"
      },
      "source": [
        "# convert the true labels batches into a long vector\n",
        "true_label = np.concatenate(labels, axis=0 )\n",
        "true_label = true_label.reshape(len(true_label), 1)\n",
        "print(true_label.shape)\n",
        "type(true_label)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(166, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXEasNQvhsIF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "53684cd8-89cf-4fd3-87ac-d8451536eaa5"
      },
      "source": [
        "# put the predictions and labels into the same dataset\n",
        "df = np.concatenate([pred, true_label], axis = 1)\n",
        "df = pd.DataFrame(data=df, columns=[\"preds\", \"labels\"])\n",
        "df"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.992658</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.987675</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.992896</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.992694</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.992651</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>0.992830</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>0.992942</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>0.992491</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>0.992022</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>0.992403</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>166 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        preds  labels\n",
              "0    0.992658     1.0\n",
              "1    0.987675     1.0\n",
              "2    0.992896     1.0\n",
              "3    0.992694     1.0\n",
              "4    0.992651     1.0\n",
              "..        ...     ...\n",
              "161  0.992830     1.0\n",
              "162  0.992942     1.0\n",
              "163  0.992491     1.0\n",
              "164  0.992022     1.0\n",
              "165  0.992403     1.0\n",
              "\n",
              "[166 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pjqBh76onyd-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e41e6316-0a15-4bfc-b430-4592d4a3f343"
      },
      "source": [
        "# see the total prediction accuracy\n",
        "sum((df[\"preds\"]>=0.5) == df[\"labels\"])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r7T9I_yhsIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "7cb32537-a798-4d61-a0ff-f25e596cc4d5"
      },
      "source": [
        "# find the index of the review that has the lowest predicted probabilty(of being a positive review) in true_label == 1 group. \n",
        "df.loc[df.loc[df['labels'] == 1, :].idxmin()]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.022807</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.992658</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       preds  labels\n",
              "42  0.022807     1.0\n",
              "0   0.992658     1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VurVKLthsIK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1a4ef998-ae9b-469c-f5aa-5655a5afd221"
      },
      "source": [
        "# see that review\n",
        "valid_raw.iloc[19,0]\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'very cute and quality is great!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9PDrrPNhsIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "770101c3-fab4-4a57-e15d-a549343854ae"
      },
      "source": [
        "# alternatively, for all the reviews that have true_label == 1, \n",
        "# let's sort their predicted probabilities\n",
        "df.loc[df['labels'] == 1, :].sort_values('preds')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.022807</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.029856</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>0.030506</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.032977</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>0.091342</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>0.992942</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>0.992945</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.992952</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.992954</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.993001</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>127 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        preds  labels\n",
              "42   0.022807     1.0\n",
              "47   0.029856     1.0\n",
              "107  0.030506     1.0\n",
              "102  0.032977     1.0\n",
              "67   0.091342     1.0\n",
              "..        ...     ...\n",
              "162  0.992942     1.0\n",
              "117  0.992945     1.0\n",
              "27   0.992952     1.0\n",
              "7    0.992954     1.0\n",
              "97   0.993001     1.0\n",
              "\n",
              "[127 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FZooOSNTGDl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "f50ac725-4e66-4779-fb2c-d8deacc7f175"
      },
      "source": [
        "df[df['labels']==1].preds.sort_values()[0:20]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42     0.022807\n",
              "47     0.029856\n",
              "107    0.030506\n",
              "102    0.032977\n",
              "67     0.091342\n",
              "29     0.142275\n",
              "51     0.286563\n",
              "64     0.287584\n",
              "110    0.365407\n",
              "37     0.416454\n",
              "10     0.444427\n",
              "98     0.609578\n",
              "131    0.632105\n",
              "136    0.719472\n",
              "115    0.923206\n",
              "92     0.924024\n",
              "22     0.933706\n",
              "126    0.935800\n",
              "58     0.949276\n",
              "106    0.964320\n",
              "Name: preds, dtype: float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wslamubJhsIQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "def2af1e-6ac3-42e0-c08e-654350cae905"
      },
      "source": [
        "# see the reviews\n",
        "valid_raw.iloc[23,0]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"comes in handy, but the rewind is a bit awkward and doesn't seem to fit right.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otzAxck1hsIR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65451851-4ef7-4c23-f772-491b3d478757"
      },
      "source": [
        "# on the other way around, for all the reviews whose label == 0, \n",
        "# let's sort their predicted probablities in descending order\n",
        "df[df['labels'] == 0].sort_values('preds', ascending = False)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.992785</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>0.992730</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>0.992493</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.992321</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>0.991955</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>0.991892</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>0.990968</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>0.988661</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.988312</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>0.982794</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>0.981416</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>0.977306</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>0.975379</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>0.967135</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>0.961751</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.960138</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>0.949335</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>0.914964</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.873575</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.871443</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>0.860366</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>0.769853</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>0.746824</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>0.561633</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>0.539557</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>0.516886</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>0.502550</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>0.475683</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.389539</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>0.286563</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.207972</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.193714</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0.147461</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>0.140877</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>0.097295</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>0.069126</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.046583</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.033220</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>0.023711</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        preds  labels\n",
              "6    0.992785     0.0\n",
              "68   0.992730     0.0\n",
              "158  0.992493     0.0\n",
              "101  0.992321     0.0\n",
              "66   0.991955     0.0\n",
              "157  0.991892     0.0\n",
              "154  0.990968     0.0\n",
              "86   0.988661     0.0\n",
              "99   0.988312     0.0\n",
              "149  0.982794     0.0\n",
              "147  0.981416     0.0\n",
              "57   0.977306     0.0\n",
              "121  0.975379     0.0\n",
              "75   0.967135     0.0\n",
              "156  0.961751     0.0\n",
              "46   0.960138     0.0\n",
              "135  0.949335     0.0\n",
              "63   0.914964     0.0\n",
              "55   0.873575     0.0\n",
              "31   0.871443     0.0\n",
              "62   0.860366     0.0\n",
              "119  0.769853     0.0\n",
              "108  0.746824     0.0\n",
              "109  0.561633     0.0\n",
              "73   0.539557     0.0\n",
              "155  0.516886     0.0\n",
              "87   0.502550     0.0\n",
              "130  0.475683     0.0\n",
              "79   0.389539     0.0\n",
              "105  0.286563     0.0\n",
              "43   0.207972     0.0\n",
              "11   0.193714     0.0\n",
              "74   0.147461     0.0\n",
              "94   0.140877     0.0\n",
              "81   0.097295     0.0\n",
              "85   0.069126     0.0\n",
              "45   0.046583     0.0\n",
              "93   0.033220     0.0\n",
              "132  0.023711     0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjDkJr_VhsIU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "2527dd34-4117-48ae-c541-dd1cbd8dffc1"
      },
      "source": [
        "# see the reviews\n",
        "# after learning some examples, it seems that our model will give a high score as long as the food is good\n",
        "# even though the service is not. \n",
        "valid_raw.iloc[26,0]"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I got this program for free with my Visor Edge, and although it was a great freebie the Edge already came with a datebook the was equally as good as this one. Still for the time I had it, it worked great and didn't give me any problems while transfering my addresses and other pieces of data from my computer to my handheld. So I recomend this to someone who has an old PDA that doesn't have a great datebook.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    }
  ]
}